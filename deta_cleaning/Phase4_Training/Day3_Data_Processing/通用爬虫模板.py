"""
é€šç”¨çˆ¬è™«æ¨¡æ¿ - å¯ä»¥ä¿®æ”¹ä¸ºçˆ¬å–ä»»ä½•ç½‘ç«™
ä½¿ç”¨è¯´æ˜ï¼š
1. ä¿®æ”¹ base_url ä¸ºç›®æ ‡ç½‘ç«™
2. ä¿®æ”¹ parse_page æ–¹æ³•ä¸­çš„é€‰æ‹©å™¨
3. ä¿®æ”¹ crawl æ–¹æ³•ä¸­çš„åˆ†é¡µé€»è¾‘
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import json
from datetime import datetime

class GenericWebCrawler:
    """é€šç”¨ç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, base_url, target_name="æ•°æ®"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å‚æ•°:
            base_url: ç›®æ ‡ç½‘ç«™çš„URL
            target_name: çˆ¬å–ç›®æ ‡çš„åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        """
        # ========== åœ¨è¿™é‡Œä¿®æ”¹ç›®æ ‡ç½‘ç«™ ==========
        self.base_url = base_url
        self.target_name = target_name
        # =======================================
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.data_list = []
        
    def get_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return response.text
        except requests.RequestException as e:
            print(f"  âœ— è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def parse_page(self, html):
        """
        è§£æé¡µé¢æ•°æ®
        
        ========== é‡è¦ï¼šæ ¹æ®ç›®æ ‡ç½‘ç«™ä¿®æ”¹è¿™é‡Œ ==========
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç¤ºä¾‹1: çˆ¬å–åˆ—è¡¨é¡¹
        # items = soup.find_all('div', class_='item-class')
        
        # ç¤ºä¾‹2: çˆ¬å–è¡¨æ ¼
        # items = soup.find('table').find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
        
        # ç¤ºä¾‹3: çˆ¬å–æ–‡ç« 
        # items = soup.find_all('article')
        
        # ========== ä¿®æ”¹é€‰æ‹©å™¨ ==========
        items = soup.find_all('div', class_='your-target-class')
        # ================================
        
        page_data = []
        for item in items:
            try:
                # ========== æ ¹æ®éœ€è¦æå–çš„å­—æ®µä¿®æ”¹ ==========
                data = {
                    'æ ‡é¢˜': item.find('h2').text.strip() if item.find('h2') else '',
                    'å†…å®¹': item.find('p').text.strip() if item.find('p') else '',
                    'é“¾æ¥': item.find('a')['href'] if item.find('a') else '',
                    # æ·»åŠ æ›´å¤šå­—æ®µ...
                }
                # ==========================================
                
                page_data.append(data)
                
            except Exception as e:
                print(f"  âœ— è§£æé¡¹ç›®å¤±è´¥: {e}")
                continue
        
        return page_data
    
    def crawl(self, num_pages=5, delay=1):
        """
        æ‰§è¡Œçˆ¬å–
        
        å‚æ•°:
            num_pages: è¦çˆ¬å–çš„é¡µæ•°
            delay: æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        print(f"\nå¼€å§‹çˆ¬å– {self.target_name}...")
        print(f"ç›®æ ‡ç½‘ç«™: {self.base_url}")
        print(f"è®¡åˆ’çˆ¬å–: {num_pages} é¡µ")
        print("=" * 60)
        
        for page in range(num_pages):
            # ========== æ ¹æ®ç½‘ç«™çš„åˆ†é¡µæ–¹å¼ä¿®æ”¹ ==========
            
            # æ–¹å¼1: ä½¿ç”¨startå‚æ•°ï¼ˆå¦‚è±†ç“£ï¼‰
            # url = f"{self.base_url}?start={page * 25}"
            
            # æ–¹å¼2: ä½¿ç”¨pageå‚æ•°
            # url = f"{self.base_url}?page={page + 1}"
            
            # æ–¹å¼3: ä½¿ç”¨è·¯å¾„å‚æ•°
            # url = f"{self.base_url}/page/{page + 1}"
            
            # æ–¹å¼4: å›ºå®šURLï¼ˆä¸åˆ†é¡µï¼‰
            url = self.base_url
            # ===========================================
            
            print(f"\næ­£åœ¨çˆ¬å–ç¬¬ {page + 1}/{num_pages} é¡µ...")
            print(f"URL: {url}")
            
            # è·å–é¡µé¢
            html = self.get_page(url)
            if not html:
                print(f"  âœ— ç¬¬{page + 1}é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # è§£ææ•°æ®
            page_data = self.parse_page(html)
            self.data_list.extend(page_data)
            print(f"  âœ“ æˆåŠŸçˆ¬å– {len(page_data)} æ¡æ•°æ®")
            
            # ç¤¼è²Œæ€§å»¶è¿Ÿ
            if page < num_pages - 1:
                time.sleep(delay)
        
        print(f"\nâœ“ çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.data_list)} æ¡æ•°æ®")
        return self.data_list
    
    def save_data(self, output_dir='Day3_Data_Processing', filename='crawled_data'):
        """ä¿å­˜æ•°æ®"""
        if not self.data_list:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        print("\nä¿å­˜æ•°æ®...")
        
        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame(self.data_list)
        csv_file = f'{output_dir}/{filename}.csv'
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"  âœ“ å·²ä¿å­˜CSV: {csv_file}")
        
        # ä¿å­˜ä¸ºJSON
        json_file = f'{output_dir}/{filename}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.data_list, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ å·²ä¿å­˜JSON: {json_file}")
        
        # ç”Ÿæˆç®€å•æŠ¥å‘Š
        self.generate_report(df, output_dir, filename)
    
    def generate_report(self, df, output_dir, filename):
        """ç”Ÿæˆæ•°æ®æŠ¥å‘Š"""
        report = f"""
{'=' * 60}
{self.target_name} çˆ¬å–æŠ¥å‘Š
{'=' * 60}

çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç›®æ ‡ç½‘ç«™: {self.base_url}
æ•°æ®æ€»é‡: {len(df)} æ¡

æ•°æ®å­—æ®µ: {', '.join(df.columns.tolist())}

{'=' * 60}
"""
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f'{output_dir}/{filename}_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("é€šç”¨çˆ¬è™«æ¨¡æ¿ - ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # ========== åœ¨è¿™é‡Œé…ç½®æ‚¨çš„çˆ¬è™« ==========
    
    # ç¤ºä¾‹1: çˆ¬å–è±†ç“£ç”µå½±
    crawler = GenericWebCrawler(
        base_url="https://www.bilibili.com/v/popular/rank/all",
        target_name="å“”å“©å“”å“©æ’è¡Œæ¦œ"
    )
    
    # ç¤ºä¾‹2: çˆ¬å–å…¶ä»–ç½‘ç«™ï¼ˆå–æ¶ˆæ³¨é‡Šä½¿ç”¨ï¼‰
    # crawler = GenericWebCrawler(
    #     base_url="https://ä½ çš„ç›®æ ‡ç½‘ç«™.com",
    #     target_name="ç½‘ç«™åç§°"
    # )
    
    # =======================================
    
    # æ‰§è¡Œçˆ¬å–
    data = crawler.crawl(
        num_pages=3,    # çˆ¬å–é¡µæ•°
        delay=1         # å»¶è¿Ÿç§’æ•°
    )
    
    # ä¿å­˜æ•°æ®
    if data:
        crawler.save_data(
            output_dir='Day3_Data_Processing',
            filename='my_crawled_data'
        )
        print("\nâœ… çˆ¬è™«ä»»åŠ¡å®Œæˆï¼")
    else:
        print("\nâœ— çˆ¬å–å¤±è´¥ï¼Œæœªè·å–åˆ°æ•°æ®")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. ä¿®æ”¹ base_url ä¸ºæ‚¨çš„ç›®æ ‡ç½‘ç«™")
    print("2. ä¿®æ”¹ parse_page æ–¹æ³•ä¸­çš„é€‰æ‹©å™¨")
    print("3. ä¿®æ”¹ crawl æ–¹æ³•ä¸­çš„URLæ„å»ºæ–¹å¼")
    print("4. è¿è¡Œè„šæœ¬å¼€å§‹çˆ¬å–")
    print("=" * 60)
