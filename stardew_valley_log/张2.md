<!--第二阶段重点：1、改进可视化质量 2、爬虫入门（详细学习在第三阶段） 3、多方向数据项目 4、继续推进Git/GitHub-->

# 美赛第二阶段训练 - 代码手实操指南

**核心职责：** 高质量可视化 + 关键信息提取 + 多方向数据项目 + 爬虫入门 + Git协作  
**时间安排：** 3天，每天4小时，共12小时  
**前置条件：** 已完成第一阶段任务，掌握基础数据处理和可视化

---

## 第一阶段问题回顾与改进

### 已达成目标 ✅
- [x] 数据收集和清洗能力
- [x] 基础可视化能力
- [x] 代码规范意识

### 需要改进的问题 ⚠️

**问题1：可视化质量不足**
- 表现：关键数据信息缺失，图表种类少
- 改进：增加图表类型，突出关键信息，添加数据标注

**问题2：缺少关键数字特征**
- 表现：没有给出关键统计信息
- 改进：计算并展示均值、中位数、标准差、相关性等

**问题3：数据获取方式单一**
- 表现：只从公开数据集下载
- 改进：爬虫入门（详细学习在第三阶段），现阶段主要从公开数据集和简单网站获取数据

**问题4：Git/GitHub使用不够**
- 表现：协作流程不熟练
- 改进：加强Git操作，建立协作流程

---

## Day 1：可视化质量提升（4小时）

### 任务1.1：关键信息可视化（2小时）

#### 做什么
在现有图表基础上，添加关键数值标注和统计信息，让图表信息更完整、更专业。

#### 用什么做
- **工具库：** Matplotlib, NumPy, Pandas
- **关键函数：** `ax.text()`（添加文本标注）、`ax.annotate()`（添加箭头标注）、`ax.axhline()`（添加参考线）

#### 怎么做

**1. 柱状图添加数值标注**
- **步骤1：** 在柱状图每个柱子上方添加数值标签
  - 使用`ax.text()`函数，位置在柱子顶部
  - 格式化为2位小数
- **步骤2：** 添加平均值参考线
  - 使用`ax.axhline()`绘制水平线
  - 在图例中标注平均值
- **步骤3：** 保存高清图片（300 DPI）

**2. 散点图添加趋势线和统计信息**
- **步骤1：** 计算并绘制趋势线
  - 使用`np.polyfit()`拟合线性趋势
  - 用虚线绘制趋势线
- **步骤2：** 计算统计指标
  - 相关系数（`np.corrcoef()`）
  - 均值、标准差（`np.mean()`, `np.std()`）
- **步骤3：** 在图表上添加统计信息文本框
  - 使用`ax.text()`在图表角落添加文本框
  - 包含相关性、均值、标准差等关键信息

**3. 时间序列图添加关键点标注**
- **步骤1：** 标注最大值和最小值
  - 使用`ax.annotate()`添加箭头标注
  - 标注具体数值
- **步骤2：** 添加平均值参考线
  - 用虚线表示平均值
- **步骤3：** 添加统计信息框
  - 显示均值、标准差、最值等

#### 实操任务
- [ ] 改进之前的空气质量可视化，添加关键数值标注
- [ ] 在图表中添加统计信息（均值、标准差、最值等）
- [ ] 添加趋势线和相关性信息
- [ ] 标注关键数据点

---

### 任务1.2：扩展图表类型（1.5小时）

#### 做什么
学习并实现新的图表类型，丰富数据可视化手段，至少新增3种之前未使用的图表类型。

#### 用什么做
- **工具库：** Matplotlib, Seaborn, NumPy, Pandas
- **关键函数：** `sns.heatmap()`（热力图）、`ax.boxplot()`（箱线图）、`sns.violinplot()`（小提琴图）、`ax.stackplot()`（堆叠面积图）

#### 怎么做

**1. 热力图（相关性矩阵）**
- **步骤1：** 计算数据相关性矩阵
  - 使用`df.corr()`计算各列之间的相关系数
- **步骤2：** 绘制热力图
  - 使用`sns.heatmap()`函数
  - 设置颜色映射（`cmap='coolwarm'`）
  - 显示数值标注（`annot=True`）
- **步骤3：** 美化图表
  - 添加标题和标签
  - 调整颜色条位置和大小

**2. 箱线图（数据分布）**
- **步骤1：** 准备数据
  - 选择要对比的多个数值列
- **步骤2：** 绘制箱线图
  - 使用`ax.boxplot()`或`plt.boxplot()`
  - 设置不同颜色区分各组
- **步骤3：** 添加统计信息
  - 在图表旁边添加文本框
  - 显示均值、中位数、四分位数等

**3. 小提琴图（分布形状）**
- **步骤1：** 使用Seaborn绘制
  - 使用`sns.violinplot()`函数
- **步骤2：** 展示数据分布密度
  - 比箱线图更详细地展示分布形状

**4. 堆叠面积图（时间序列多变量）**
- **步骤1：** 准备时间序列数据
  - 多个变量随时间变化的数据
- **步骤2：** 绘制堆叠图
  - 使用`ax.stackplot()`函数
  - 设置透明度便于观察
- **步骤3：** 添加图例和标签

**5. 雷达图（多维度对比）**
- **步骤1：** 准备多维度数据
  - 每个维度一个数值
- **步骤2：** 使用极坐标系统
  - 使用`subplot_kw=dict(projection='polar')`
- **步骤3：** 绘制并填充
  - 连接各点形成多边形

#### 实操任务
- [ ] 为空气质量数据创建相关性热力图
- [ ] 创建箱线图展示各污染物分布
- [ ] 创建堆叠面积图展示时间趋势
- [ ] 至少新增3种之前未使用的图表类型

---

### 任务1.3：数据特征提取与展示（0.5小时）

#### 做什么
计算并展示数据的关键统计特征，包括均值、中位数、标准差、最值、四分位数等，生成统计报告。

#### 用什么做
- **工具库：** Pandas, NumPy
- **关键函数：** `df.describe()`（描述性统计）、`df.mean()`、`df.std()`、`df.quantile()`等

#### 怎么做

**1. 计算关键统计信息**
- **步骤1：** 选择数值列
  - 识别数据中的数值型列
- **步骤2：** 计算统计指标
  - 计数（count）
  - 均值（mean）
  - 中位数（median）
  - 标准差（std）
  - 最小值、最大值（min, max）
  - 四分位数（Q1, Q3）
  - 偏度、峰度（skewness, kurtosis）
- **步骤3：** 整理成表格
  - 使用DataFrame存储统计结果
  - 每列一个指标，每行一个变量

**2. 保存统计报告**
- **步骤1：** 导出为CSV
  - 使用`df.to_csv()`保存
- **步骤2：** 格式化输出
  - 保留2位小数
  - 添加标题和说明

**3. 可视化统计信息**
- **步骤1：** 创建多子图
  - 2×2布局展示不同统计指标
- **步骤2：** 绘制对比图
  - 均值对比（柱状图）
  - 标准差对比（柱状图）
  - 数值范围对比（水平柱状图）
  - 四分位距对比（水平柱状图）

#### 实操任务
- [ ] 计算空气质量数据的关键统计信息
- [ ] 创建统计信息表格并保存为CSV
- [ ] 可视化统计信息
- [ ] 在报告中展示关键数字特征

---

## Day 2：爬虫入门 + 多方向项目开始（4小时）

### 任务2.1：爬虫入门（1.5小时）

#### 做什么
爬虫入门学习，了解基本概念和简单操作。主要学习静态网页数据提取，为第三阶段的深入学习打基础。

#### 用什么做
- **工具库：** 
  - `requests` - 发送HTTP请求
  - `BeautifulSoup4` - 解析HTML
  - `lxml` - HTML解析器
  - `pandas` - 数据处理
- **安装命令：** `pip install requests beautifulsoup4 lxml pandas`

#### 怎么做

**1. 简单网页爬取**
- **步骤1：** 发送HTTP请求
  - 使用`requests.get(url)`获取网页内容
  - 设置请求头（User-Agent）模拟浏览器，避免被拒绝
  - 设置编码为UTF-8处理中文
- **步骤2：** 解析HTML
  - 使用`BeautifulSoup(response.text, 'lxml')`解析HTML
  - 获得可操作的文档对象
- **步骤3：** 提取数据
  - 使用`soup.find()`或`soup.find_all()`查找元素
  - 通过标签名、class、id等定位元素
  - 提取文本内容（`.text`）

**2. 表格数据爬取**
- **步骤1：** 定位表格
  - 使用`soup.find('table')`找到表格元素
  - 如果有table_id，使用id定位更精确
- **步骤2：** 提取表头
  - 找到所有`<th>`标签，提取表头文本
- **步骤3：** 提取数据行
  - 遍历所有`<tr>`标签（跳过表头行）
  - 提取每行的`<td>`标签内容
- **步骤4：** 转换为DataFrame
  - 使用`pd.DataFrame()`创建数据框
  - 保存为CSV文件

**注意事项：**
- 遵守网站的robots.txt规则
- 控制爬取频率，不要对服务器造成压力
- 处理异常情况（网络错误、页面不存在等）
- 检查数据合法性

**说明：** 本阶段只学习基础爬虫，动态网页爬取（Selenium）和API数据获取将在第三阶段深入学习。

#### 实操任务
- [ ] 安装爬虫相关库（requests, beautifulsoup4, lxml）
- [ ] 尝试爬取一个简单的静态网页
- [ ] 提取表格数据并保存为CSV
- [ ] 了解爬虫的基本流程

---

### 任务2.2：多方向项目1 - 数据获取和清洗（2.5小时）

#### 做什么
对爬取到的原始数据进行清洗和格式化，然后保存到指定目录。

#### 用什么做
- **工具库：** Pandas
- **关键函数：** `drop_duplicates()`、`dropna()`、`to_numeric()`、`to_datetime()`、`str.strip()`

#### 怎么做

**1. 数据清洗步骤**
- **步骤1：** 删除重复数据
  - 使用`df.drop_duplicates()`删除完全重复的行
- **步骤2：** 处理缺失值
  - 检查缺失值情况（`df.isnull().sum()`）
  - 删除关键列缺失的行（`df.dropna(subset=['关键列'])`）
  - 或填充缺失值（根据实际情况）
- **步骤3：** 数据类型转换
  - 数值列：使用`pd.to_numeric()`转换，设置`errors='coerce'`处理无法转换的值
  - 日期列：使用`pd.to_datetime()`转换
- **步骤4：** 文本清理
  - 去除前后空格：使用`df[col].str.strip()`
  - 去除特殊字符（如换行符、制表符）
- **步骤5：** 处理异常值
  - 识别异常值（如负数、超出合理范围的值）
  - 删除或替换异常值

**2. 数据存储**
- **步骤1：** 创建目录结构
  - `data/raw/` - 存放原始数据
  - `data/processed/` - 存放清洗后数据
- **步骤2：** 保存为CSV
  - 使用`df.to_csv()`保存
  - 设置`encoding='utf-8-sig'`支持中文
  - 设置`index=False`不保存索引
- **步骤3：** 可选保存为Excel
  - 使用`df.to_excel()`保存
  - 便于查看和编辑

**3. 创建数据说明文档**
- **步骤1：** 记录数据来源
  - 网站URL、爬取时间
- **步骤2：** 记录数据字段
  - 列名、数据类型、含义
- **步骤3：** 记录数据量
  - 行数、列数、时间范围
- **步骤4：** 记录处理步骤
  - 清洗了哪些内容、删除了多少数据

#### 实操任务
- [ ] 清洗爬取的数据
- [ ] 保存到指定目录
- [ ] 创建数据说明文档

---

## Day 3：多方向数据项目完成（4小时）

### 任务3.1：项目1 - 可视化和统计分析（2小时）

#### 做什么
完成项目1（Day 2已获取和清洗的数据）的可视化和统计分析工作。

#### 用什么做
- **工具库：** Matplotlib, Seaborn, Pandas, NumPy
- **图表类型：** 至少5种不同类型（热力图、箱线图、堆叠图、时间序列等）

#### 怎么做

**1. 数据可视化（约1.5小时）**
- **步骤1：** 创建多种图表
  - 至少5种不同类型的图表
  - 使用Day 1学习的图表类型
  - 包含热力图、箱线图、堆叠面积图等
- **步骤2：** 添加关键信息
  - 数值标注
  - 统计信息（均值、标准差等）
  - 趋势线、相关性信息
- **步骤3：** 美化图表
  - 统一风格
  - 高清保存（≥300 DPI）
  - 保存到`figures/`目录

**2. 统计分析（约0.5小时）**
- **步骤1：** 计算关键统计指标
  - 均值、中位数、标准差、最值、四分位数等
- **步骤2：** 生成统计报告
  - 整理成表格
  - 保存为CSV文件
- **步骤3：** 数据洞察
  - 总结关键发现
  - 记录在文档中

#### 实操任务
- [ ] 为项目1数据创建至少5种不同类型的图表
- [ ] 所有图表包含关键数值标注和统计信息
- [ ] 生成统计报告
- [ ] 图表质量达标（300 DPI）

---

### 任务3.2：项目2 - 完整流程（1.5小时）

#### 做什么
完成第二个新方向（如经济数据）的完整数据项目，包括数据获取、清洗、可视化和统计分析。

#### 用什么做
- **数据来源：** 公开数据集或简单网站
- **工具库：** 与项目1相同

#### 怎么做

**1. 数据获取和清洗（约40分钟）**
- **步骤1：** 选择第二个数据方向
  - 与项目1不同的方向（如经济数据、健康数据、能源数据等）
- **步骤2：** 获取数据
  - 从公开数据集下载（推荐）
  - 或使用简单爬虫获取
- **步骤3：** 数据清洗
  - 删除重复值、处理缺失值
  - 数据类型转换、异常值处理
  - 保存清洗后数据

**2. 数据可视化（约40分钟）**
- **步骤1：** 创建多种图表
  - 至少5种不同类型的图表
  - 添加关键数值标注和统计信息
- **步骤2：** 保存高质量图片
  - 分辨率≥300 DPI

**3. 统计分析（约10分钟）**
- **步骤1：** 计算关键统计指标
- **步骤2：** 生成统计报告
- **步骤3：** 数据洞察分析

**注意：** 两个项目应该选择不同的数据方向，展示多样化的数据获取和处理能力。

#### 实操任务
- [ ] 完成方向2的完整流程
- [ ] 代码规范，有注释
- [ ] 生成可视化图表（至少5种类型）
- [ ] 生成统计报告

---

### 任务3.4：Git/GitHub协作（0.5小时）

#### 常用Git命令复习

```bash
# 基本操作
git status                    # 查看状态
git add .                     # 添加所有文件
git commit -m "message"       # 提交
git log --oneline            # 查看历史
git diff                     # 查看差异

# 分支操作
git branch                   # 查看分支
git branch feature-name      # 创建分支
git checkout feature-name    # 切换分支
git merge feature-name       # 合并分支

# 远程操作
git remote -v                # 查看远程仓库
git push origin main         # 推送
git pull origin main         # 拉取
git clone url                # 克隆仓库
```

#### 协作工作流

**1. 日常协作流程**

```bash
# 每天开始工作前
git pull origin main          # 拉取最新代码

# 创建功能分支
git checkout -b feature/data-visualization

# 工作完成后
git add .
git commit -m "[Viz] 添加空气质量可视化图表"
git push origin feature/data-visualization

# 在主分支合并
git checkout main
git pull origin main
git merge feature/data-visualization
git push origin main
```

**2. 解决冲突**

```bash
# 当出现冲突时
git pull origin main

# 手动解决冲突后
git add .
git commit -m "[Fix] 解决合并冲突"
git push origin main
```

#### 提交信息规范

```
[类型] 简短描述

详细描述（可选）

类型：
[Data] - 数据相关
[Crawl] - 爬虫相关
[Clean] - 数据清洗
[Viz] - 可视化
[Stats] - 统计分析
[Fix] - 修复bug
[Doc] - 文档更新
[Refactor] - 代码重构
```

**示例：**
```bash
git commit -m "[Crawl] 添加空气质量数据爬虫"
git commit -m "[Viz] 完成相关性热力图，添加统计信息"
git commit -m "[Stats] 计算并导出关键统计指标"
```

#### Git协作要点

**基本工作流：**
```bash
# 每天开始工作前
git pull origin main

# 工作完成后
git add .
git commit -m "[类型] 描述"
git push origin main
```

**提交信息规范：**
- `[Data]` - 数据相关
- `[Crawl]` - 爬虫相关
- `[Viz]` - 可视化
- `[Stats]` - 统计分析
- `[Fix]` - 修复bug

#### 实操任务
- [ ] 将完成的项目提交到GitHub
- [ ] 使用规范的提交信息
- [ ] 确保代码和文档都上传

---


---

## ✅ 第二阶段完成标准

完成以下所有任务即可进入第三阶段：

### 可视化能力
- [ ] 图表类型≥10种
- [ ] 所有图表包含关键数值标注
- [ ] 所有图表包含统计信息
- [ ] 图表质量≥300 DPI

### 数据获取能力
- [ ] 了解基础爬虫概念（requests + BeautifulSoup入门）
- [ ] 能够从公开数据集获取数据
- [ ] 能够尝试简单网站数据爬取
- [ ] 完成至少2个方向的数据项目（空气质量改进 + 2个新方向）

### 统计分析能力
- [ ] 能够计算关键统计指标
- [ ] 能够生成统计报告
- [ ] 能够提取数据洞察

### Git协作能力
- [ ] 掌握Git基本操作
- [ ] 能够提交代码到GitHub
- [ ] 提交信息规范

### 项目完成度
- [ ] 完成空气质量数据项目可视化改进（添加关键信息、扩展图表类型）
- [ ] 完成项目1数据项目（新方向，完整流程）
- [ ] 完成项目2数据项目（新方向，完整流程）
- [ ] 所有项目代码规范，有注释
- [ ] 所有项目有数据说明文档

---

## 📚 学习资源

### 数据可视化
- Matplotlib官方文档
- Seaborn官方文档
- Plotly文档
- B站：Python数据可视化教程

### 网络爬虫
- BeautifulSoup官方文档
- Selenium官方文档
- 爬虫教程：https://github.com/NanmiCoder/CrawlerTutorial?tab=readme-ov-file
- B站：Python爬虫教程

### Git/GitHub
- 廖雪峰Git教程
- GitHub官方指南
- B站：Git入门教程

